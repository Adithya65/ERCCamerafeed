{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3a372dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "4200/4200 [==============================] - 59s 14ms/step - loss: 1.9948 - accuracy: 0.4973 - val_loss: 1.0824 - val_accuracy: 0.6308\n",
      "Epoch 2/5\n",
      "4200/4200 [==============================] - 62s 15ms/step - loss: 0.6385 - accuracy: 0.7999 - val_loss: 0.4125 - val_accuracy: 0.8813\n",
      "Epoch 3/5\n",
      "4200/4200 [==============================] - 61s 15ms/step - loss: 0.2674 - accuracy: 0.9334 - val_loss: 0.1701 - val_accuracy: 0.9574\n",
      "Epoch 4/5\n",
      "4200/4200 [==============================] - 64s 15ms/step - loss: 0.1556 - accuracy: 0.9595 - val_loss: 0.1803 - val_accuracy: 0.9551\n",
      "Epoch 5/5\n",
      "4200/4200 [==============================] - 64s 15ms/step - loss: 0.1314 - accuracy: 0.9675 - val_loss: 0.1577 - val_accuracy: 0.9583\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2e19e08afa0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "mnist=tf.keras.datasets.mnist\n",
    "(x_train,y_train),(x_test,y_test)=mnist.load_data()\n",
    "(x_train,y_train),(x_test,y_test)=mnist.load_data()\n",
    "x_trainr=np.array(x_train).reshape(-1,28,28,1)\n",
    "x_testr=np.array(x_test).reshape(-1,28,28,1)\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,Activation,Flatten,Conv2D,MaxPooling2D\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model=Sequential()\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=x_trainr.shape[1:])) \n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=x_trainr.shape[1:])) \n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=x_trainr.shape[1:])) \n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation(\"relu\"))\n",
    " \n",
    "model.add(Dense(32))\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    " \n",
    "model.add(Dense(10))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",optimizer=\"adam\",metrics=['accuracy'])\n",
    "model.fit(x_trainr,y_train ,epochs=5,validation_split=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a231fed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 26ms/step\n"
     ]
    }
   ],
   "source": [
    "img=cv2.imread(\"c.png\")\n",
    "gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "resized=cv2.resize(gray,(28,28),interpolation=cv2.INTER_AREA)\n",
    "newimg=tf.keras.utils.normalize(resized,axis=1)\n",
    "newimg3=np.array(newimg).reshape(-1,28,28,1)\n",
    "predictions=model.predict(newimg3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9a35077e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "67f9341b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 9ms/step\n"
     ]
    }
   ],
   "source": [
    "pred=model.predict([x_testr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5773142",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9f6d545e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(pred[ 889])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9ebf93ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2e19d84ca30>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    " plt.imshow(x_testr[889])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "59e7bae3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'needed'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10388/253571171.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# from scipy.misc.pilutil import imresize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mneeded\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimresize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m  \u001b[1;31m# version 3.2.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'needed'"
     ]
    }
   ],
   "source": [
    " \n",
    "\n",
    "import numpy as np\n",
    "# from scipy.misc.pilutil import imresize\n",
    "from needed import imresize\n",
    "from PIL import Image\n",
    "import cv2  # version 3.2.0\n",
    "from skimage.feature import hog\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "DIGIT_WIDTH = 10\n",
    "DIGIT_HEIGHT = 20\n",
    "IMG_HEIGHT = 28\n",
    "IMG_WIDTH = 28\n",
    "CLASS_N = 10  # 0-9\n",
    "\n",
    "\n",
    "# This method splits the input training image into small cells (of a single digit) and uses these cells as training data.\n",
    "# The default training image (MNIST) is a 1000x1000 size image and each digit is of size 10x20. so we divide 1000/10 horizontally and 1000/20 vertically.\n",
    "def split2d(img, cell_size, flatten=True):\n",
    "    h, w = img.shape[:2]\n",
    "    sx, sy = cell_size\n",
    "    cells = [np.hsplit(row, w // sx) for row in np.vsplit(img, h // sy)]\n",
    "    cells = np.array(cells)\n",
    "    if flatten:\n",
    "        cells = cells.reshape(-1, sy, sx)\n",
    "    return cells\n",
    "\n",
    "\n",
    "def load_digits(fn):\n",
    "    print('loading \"%s for training\" ...' % fn)\n",
    "    digits_img = cv2.imread(fn, 0)\n",
    "    digits = split2d(digits_img, (DIGIT_WIDTH, DIGIT_HEIGHT))\n",
    "    resized_digits = []\n",
    "    for digit in digits:\n",
    "        resized_digits.append(imresize(digit, (IMG_WIDTH, IMG_HEIGHT)))\n",
    "    labels = np.repeat(np.arange(CLASS_N), len(digits) / CLASS_N)\n",
    "    return np.array(resized_digits), labels\n",
    "\n",
    "\n",
    "def pixels_to_hog_20(img_array):\n",
    "    hog_featuresData = []\n",
    "    for img in img_array:\n",
    "        fd = hog(img,\n",
    "                 orientations=10,\n",
    "                 pixels_per_cell=(5, 5),\n",
    "                 cells_per_block=(1, 1))\n",
    "        hog_featuresData.append(fd)\n",
    "    hog_features = np.array(hog_featuresData, 'float64')\n",
    "    return np.float32(hog_features)\n",
    "\n",
    "\n",
    "# define a custom model in a similar class wrapper with train and predict methods\n",
    "class KNN_MODEL():\n",
    "    def __init__(self, k=3):\n",
    "        self.k = k\n",
    "        self.model = cv2.ml.KNearest_create()\n",
    "\n",
    "    def train(self, samples, responses):\n",
    "        self.model.train(samples, cv2.ml.ROW_SAMPLE, responses)\n",
    "\n",
    "    def predict(self, samples):\n",
    "        retval, results, neigh_resp, dists = self.model.findNearest(samples, self.k)\n",
    "        return results.ravel()\n",
    "\n",
    "\n",
    "class SVM_MODEL():\n",
    "    def __init__(self, num_feats, C=1, gamma=0.1):\n",
    "        self.model = cv2.ml.SVM_create()\n",
    "        self.model.setType(cv2.ml.SVM_C_SVC)\n",
    "        self.model.setKernel(cv2.ml.SVM_RBF)  # SVM_LINEAR, SVM_RBF\n",
    "        self.model.setC(C)\n",
    "        self.model.setGamma(gamma)\n",
    "        self.features = num_feats\n",
    "\n",
    "    def train(self, samples, responses):\n",
    "        self.model.train(samples, cv2.ml.ROW_SAMPLE, responses)\n",
    "\n",
    "    def predict(self, samples):\n",
    "        results = self.model.predict(samples.reshape(-1, self.features))\n",
    "        return results[1].ravel()\n",
    "\n",
    "\n",
    "def get_digits(contours, hierarchy):\n",
    "    hierarchy = hierarchy[0]\n",
    "    bounding_rectangles = [cv2.boundingRect(ctr) for ctr in contours]\n",
    "    final_bounding_rectangles = []\n",
    "    # find the most common heirarchy level - that is where our digits's bounding boxes are\n",
    "    u, indices = np.unique(hierarchy[:, -1], return_inverse=True)\n",
    "    most_common_heirarchy = u[np.argmax(np.bincount(indices))]\n",
    "\n",
    "    for r, hr in zip(bounding_rectangles, hierarchy):\n",
    "        x, y, w, h = r\n",
    "        # this could vary depending on the image you are trying to predict\n",
    "        # we are trying to extract ONLY the rectangles with images in it (this is a very simple way to do it)\n",
    "        # we use heirarchy to extract only the boxes that are in the same global level - to avoid digits inside other digits\n",
    "        # ex: there could be a bounding box inside every 6,9,8 because of the loops in the number's appearence - we don't want that.\n",
    "        # read more about it here: https://docs.opencv.org/trunk/d9/d8b/tutorial_py_contours_hierarchy.html\n",
    "        if ((w * h) > 250) and (10 <= w <= 200) and (10 <= h <= 200) and hr[3] == most_common_heirarchy:\n",
    "            final_bounding_rectangles.append(r)\n",
    "\n",
    "    return final_bounding_rectangles\n",
    "\n",
    "\n",
    "def proc_user_img(img_file, model):\n",
    "    print('loading \"%s for digit recognition\" ...' % img_file)\n",
    "    im = cv2.imread(img_file)\n",
    "    blank_image = np.zeros((im.shape[0], im.shape[1], 3), np.uint8)\n",
    "    blank_image.fill(255)\n",
    "\n",
    "    imgray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "    plt.imshow(imgray)\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "\n",
    "    ret, thresh = cv2.threshold(imgray, 127, 255, 0)\n",
    "    thresh = cv2.erode(thresh, kernel, iterations=1)\n",
    "    thresh = cv2.dilate(thresh, kernel, iterations=1)\n",
    "    thresh = cv2.erode(thresh, kernel, iterations=1)\n",
    "\n",
    "    contours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    digits_rectangles = get_digits(contours, hierarchy)  # rectangles of bounding the digits in user image\n",
    "\n",
    "    for rect in digits_rectangles:\n",
    "        x, y, w, h = rect\n",
    "        cv2.rectangle(im, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        im_digit = imgray[y:y + h, x:x + w]\n",
    "        im_digit = (255 - im_digit)\n",
    "        im_digit = imresize(im_digit, (IMG_WIDTH, IMG_HEIGHT))\n",
    "\n",
    "        hog_img_data = pixels_to_hog_20([im_digit])\n",
    "        pred = model.predict(hog_img_data)\n",
    "        cv2.putText(im, str(int(pred[0])), (x, y), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 0, 0), 3)\n",
    "        cv2.putText(blank_image, str(int(pred[0])), (x, y), cv2.FONT_HERSHEY_SIMPLEX, 3, (255, 0, 0), 5)\n",
    "\n",
    "    plt.imshow(im)\n",
    "    cv2.imwrite(\"original_overlay.png\", im)\n",
    "    cv2.imwrite(\"final_digits.png\", blank_image)\n",
    "    # cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "def get_contour_precedence(contour, cols):\n",
    "    return contour[1] * cols + contour[0]  # row-wise ordering\n",
    "\n",
    "\n",
    "# this function processes a custom training image\n",
    "# see example : custom_train.digits.jpg\n",
    "# if you want to use your own, it should be in a similar format\n",
    "def load_digits_custom(img_file):\n",
    "    train_data = []\n",
    "    train_target = []\n",
    "    start_class = 1\n",
    "    im = cv2.imread(img_file)\n",
    "    imgray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "    plt.imshow(imgray)\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "\n",
    "    ret, thresh = cv2.threshold(imgray, 127, 255, 0)\n",
    "    thresh = cv2.erode(thresh, kernel, iterations=1)\n",
    "    thresh = cv2.dilate(thresh, kernel, iterations=1)\n",
    "    thresh = cv2.erode(thresh, kernel, iterations=1)\n",
    "\n",
    "    contours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    digits_rectangles = get_digits(contours, hierarchy)  # rectangles of bounding the digits in user image\n",
    "\n",
    "    # sort rectangles accoring to x,y pos so that we can label them\n",
    "    digits_rectangles.sort(key=lambda x: get_contour_precedence(x, im.shape[1]))\n",
    "\n",
    "    for index, rect in enumerate(digits_rectangles):\n",
    "        x, y, w, h = rect\n",
    "        cv2.rectangle(im, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        im_digit = imgray[y:y + h, x:x + w]\n",
    "        im_digit = (255 - im_digit)\n",
    "\n",
    "        im_digit = imresize(im_digit, (IMG_WIDTH, IMG_HEIGHT))\n",
    "        train_data.append(im_digit)\n",
    "        train_target.append(start_class % 10)\n",
    "\n",
    "        if index > 0 and (index + 1) % 10 == 0:\n",
    "            start_class += 1\n",
    "    cv2.imwrite(\"training_box_overlay.png\", im)\n",
    "\n",
    "    return np.array(train_data), np.array(train_target)\n",
    "\n",
    "\n",
    "# ------------------data preparation--------------------------------------------\n",
    "\n",
    "TRAIN_MNIST_IMG = 'digits.png'\n",
    "TRAIN_USER_IMG = 'custom_train_digits.jpg'\n",
    "TEST_USER_IMG = 'test_image.png'\n",
    "\n",
    "# digits, labels = load_digits(TRAIN_MNIST_IMG) #original MNIST data (not good detection)\n",
    "digits, labels = load_digits_custom(\n",
    "    TRAIN_USER_IMG)  # my handwritten dataset (better than MNIST on my handwritten digits)\n",
    "\n",
    "print('train data shape', digits.shape)\n",
    "print('test data shape', labels.shape)\n",
    "\n",
    "digits, labels = shuffle(digits, labels, random_state=256)\n",
    "train_digits_data = pixels_to_hog_20(digits)\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_digits_data, labels, test_size=0.33, random_state=42)\n",
    "\n",
    "# ------------------training and testing----------------------------------------\n",
    "\n",
    "model = KNN_MODEL(k=3)\n",
    "model.train(X_train, y_train)\n",
    "preds = model.predict(X_test)\n",
    "print('Accuracy: ', accuracy_score(y_test, preds))\n",
    "\n",
    "model = KNN_MODEL(k=4)\n",
    "model.train(train_digits_data, labels)\n",
    "proc_user_img(TEST_USER_IMG, model)\n",
    "\n",
    "model = SVM_MODEL(num_feats=train_digits_data.shape[1])\n",
    "model.train(X_train, y_train)\n",
    "preds = model.predict(X_test)\n",
    "print('Accuracy: ', accuracy_score(y_test, preds))\n",
    "\n",
    "model = SVM_MODEL(num_feats=train_digits_data.shape[1])\n",
    "model.train(train_digits_data, labels)\n",
    "proc_user_img(TEST_USER_IMG, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "91a90e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement needed (from versions: none)\n",
      "ERROR: No matching distribution found for needed\n"
     ]
    }
   ],
   "source": [
    "!pip install needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f493823e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 80s 42ms/step - loss: 0.1185 - accuracy: 0.9650 - val_loss: 0.0568 - val_accuracy: 0.9825\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 81s 43ms/step - loss: 0.0552 - accuracy: 0.9835 - val_loss: 0.0383 - val_accuracy: 0.9879\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 85s 45ms/step - loss: 0.0438 - accuracy: 0.9870 - val_loss: 0.0602 - val_accuracy: 0.9844\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 87s 47ms/step - loss: 0.0395 - accuracy: 0.9892 - val_loss: 0.0775 - val_accuracy: 0.9798\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 91s 49ms/step - loss: 0.0351 - accuracy: 0.9909 - val_loss: 0.0607 - val_accuracy: 0.9895\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 94s 50ms/step - loss: 0.0339 - accuracy: 0.9915 - val_loss: 0.0835 - val_accuracy: 0.9838\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 96s 51ms/step - loss: 0.0335 - accuracy: 0.9925 - val_loss: 0.0803 - val_accuracy: 0.9865\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 99s 53ms/step - loss: 0.0356 - accuracy: 0.9921 - val_loss: 0.0926 - val_accuracy: 0.9860\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 102s 54ms/step - loss: 0.0361 - accuracy: 0.9928 - val_loss: 0.0885 - val_accuracy: 0.9878\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 104s 56ms/step - loss: 0.0265 - accuracy: 0.9945 - val_loss: 0.0812 - val_accuracy: 0.9891\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.0812 - accuracy: 0.9891\n",
      "Test accuracy: 0.9890999794006348\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tfjs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10388/3370018841.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;31m# save model as tfjs format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[0mtfjs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_keras_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'models'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'tfjs' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    " \n",
    "from tensorflow import keras\n",
    "\n",
    "# load the data\n",
    "(train_img,train_label),(test_img,test_label) = keras.datasets.mnist.load_data()\n",
    "train_img = train_img.reshape([-1, 28, 28, 1])\n",
    "test_img = test_img.reshape([-1, 28, 28, 1])\n",
    "train_img = train_img/255.0\n",
    "test_img = test_img/255.0\n",
    "train_label = keras.utils.to_categorical(train_label)\n",
    "test_label = keras.utils.to_categorical(test_label)\n",
    "\n",
    "# define the model architecture\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(32, (5, 5), padding=\"same\", input_shape=[28, 28, 1]),\n",
    "    keras.layers.MaxPool2D((2,2)),\n",
    "    keras.layers.Conv2D(64, (5, 5), padding=\"same\"),\n",
    "    keras.layers.MaxPool2D((2,2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(1024, activation='relu'),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# train the model\n",
    "model.fit(train_img,train_label, validation_data=(test_img,test_label), epochs=10)\n",
    "test_loss,test_acc = model.evaluate(test_img, test_label)\n",
    "print('Test accuracy:', test_acc)\n",
    "\n",
    "# save model as tfjs format\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f85deed",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
